{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Demo\n",
        "\n",
        "# For running in google colab, I have to upload the files to the running session folder first, such as Dict, BaseModel and NewModel, but after doing that, the code should work fine."
      ],
      "metadata": {
        "id": "h4DZhYqsdBVI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xvcIQUG4PKWp"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading tokenizer and vocab\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = torch.load('./Dict.zip')\n",
        "\n",
        "#label dict \n",
        "label_dict = {\n",
        "    0: \"fear\",\n",
        "    1: \"sadness\",\n",
        "    2: \"anger\", \n",
        "    3: \"disgust\", \n",
        "    4:\"contentment\", \n",
        "    5: \"awe\", \n",
        "    6: \"something else\",\n",
        "    7: \"amusement\", \n",
        "    8: \"excitement\"\n",
        "}"
      ],
      "metadata": {
        "id": "7fb7HwwDQ7iI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Model"
      ],
      "metadata": {
        "id": "SD44VAX7dFjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, ff_dim, dropout):\n",
        "\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        \n",
        "        self.linear_layer_1 = nn.Linear(input_dim, ff_dim)\n",
        "        self.activation_layer = nn.LeakyReLU()\n",
        "        self.dropout_layer = nn.Dropout(p = dropout)\n",
        "        self.linear_layer_2 = nn.Linear(ff_dim, input_dim)\n",
        "\n",
        "        \n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \n",
        "        y = None\n",
        "\n",
        "        output1 = self.linear_layer_1(x)\n",
        "        output2 = self.activation_layer(output1)\n",
        "        output3 = self.dropout_layer(output2)\n",
        "        y = self.linear_layer_2(output3)\n",
        "        \n",
        "        return y\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.0, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, dropout: float):\n",
        "\n",
        "        super(TransformerCell, self).__init__()\n",
        "        \n",
        "        self.multi_attention = nn.MultiheadAttention(input_dim, num_heads, batch_first=True)\n",
        "        self.dropout_layer_1 = nn.Dropout(dropout)\n",
        "        self.batchnorm_layer_1 = nn.LayerNorm(input_dim)\n",
        "\n",
        "        self.feedfoward_layer = FeedForwardNetwork(input_dim, ff_dim, dropout)\n",
        "        self.dropout_layer_2 = nn.Dropout(dropout)\n",
        "        self.batchnorm_layer_2 = nn.LayerNorm(input_dim)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor=None):\n",
        "        \n",
        "        y = None\n",
        "\n",
        "        output1, attn_output_weights = self.multi_attention(x, x, x, mask)\n",
        "        output2 = self.dropout_layer_1(output1)\n",
        "        output3 = self.batchnorm_layer_1(x + output2)\n",
        "\n",
        "        output4 = self.feedfoward_layer(output3)\n",
        "        output5 = self.dropout_layer_2(output4)\n",
        "        y = self.batchnorm_layer_2(output5 + output3)\n",
        "        \n",
        "        return y\n",
        "\n",
        "class TransformerBaseline(nn.Module):\n",
        "    \"\"\"\n",
        "    A Transformer-based text classifier.\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "            vocab_size: int, embed_dim: int, num_heads: int, trx_ff_dim: int, \n",
        "            num_trx_cells: int, num_class: int, dropout: float=0.1, pad_token: int=1\n",
        "        ):\n",
        "        super(TransformerBaseline, self).__init__()\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_token)\n",
        "        self.positional_layer = PositionalEncoding(embed_dim)\n",
        "        self.transform_layer = nn.ModuleList(TransformerCell(embed_dim, num_heads, trx_ff_dim, dropout) for i in range(num_trx_cells))\n",
        "        self.output_layer = nn.Linear(embed_dim, num_class)\n",
        "\n",
        "    def forward(self, text, mask=None):\n",
        "        embedded = self.embedding(text) * math.sqrt(self.embed_dim)\n",
        "\n",
        "        logits = None\n",
        "        positional = self.positional_layer(embedded)\n",
        "        for num, i in enumerate(self.transform_layer):\n",
        "          positional = i(positional, mask)\n",
        "        attention_output = torch.mean(positional, 1)\n",
        "        logits = self.output_layer(attention_output)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "class TransformerLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, dropout: float):\n",
        "\n",
        "        super(TransformerLSTMCell, self).__init__()\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_dim, input_dim, batch_first=True, dropout = dropout, bidirectional=True)\n",
        "        self.feedfoward_layer_lstm = FeedForwardNetwork(input_dim, ff_dim, dropout)\n",
        "\n",
        "        self.multi_attention = nn.MultiheadAttention(input_dim, num_heads, batch_first=True)\n",
        "        self.dropout_layer_1 = nn.Dropout(dropout)\n",
        "        self.batchnorm_layer_1 = nn.LayerNorm(input_dim)\n",
        "\n",
        "        self.feedfoward_layer_multi = FeedForwardNetwork(input_dim, ff_dim, dropout)\n",
        "        self.dropout_layer_2 = nn.Dropout(dropout)\n",
        "        self.batchnorm_layer_2 = nn.LayerNorm(input_dim)\n",
        "\n",
        "        \n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor=None):\n",
        "        \n",
        "        y = None\n",
        "        #print(x.shape)\n",
        "\n",
        "        output1, attn_output_weights = self.multi_attention(x, x, x, mask)\n",
        "        output2 = self.dropout_layer_1(output1)\n",
        "        output3 = self.batchnorm_layer_1(x + output2)\n",
        "\n",
        "        output, (ht, ct) = self.lstm(x)\n",
        "        #print(x.shape[-1])\n",
        "        output = output[:, :, :x.shape[-1]] + output[:, :, x.shape[-1]:]\n",
        "        output = output/2.0\n",
        "        #print(output.shape)\n",
        "        output6 = self.feedfoward_layer_lstm(output)\n",
        "        #print(output6.shape)\n",
        "\n",
        "        #print(output3.shape)\n",
        "        output4 = self.feedfoward_layer_multi(output3)\n",
        "        output5 = self.dropout_layer_2(output4)\n",
        "        #print(output5.shape)\n",
        "        y = self.batchnorm_layer_2(output5 + (output3 + output6))\n",
        "        \n",
        "        return y\n",
        "    \n",
        "class TransformerLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    A Transformer-based text classifier.\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "            vocab_size: int, embed_dim: int, num_heads: int, trx_ff_dim: int, \n",
        "            num_trx_cells: int, num_class: int, dropout: float=0.1, pad_token: int=1\n",
        "        ):\n",
        "        super(TransformerLSTM, self).__init__()\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_token)\n",
        "        self.positional_layer = PositionalEncoding(embed_dim)\n",
        "        self.transform_layer = nn.ModuleList(TransformerLSTMCell(embed_dim, num_heads, trx_ff_dim, dropout) for i in range(num_trx_cells))\n",
        "        self.output_layer = nn.Linear(embed_dim, num_class)\n",
        "\n",
        "    def forward(self, text, mask=None):\n",
        "        embedded = self.embedding(text) * math.sqrt(self.embed_dim)\n",
        "\n",
        "        logits = None\n",
        "        positional = self.positional_layer(embedded)\n",
        "        for num, i in enumerate(self.transform_layer):\n",
        "          positional = i(positional, mask)\n",
        "        attention_output = torch.mean(positional, 1)\n",
        "        logits = self.output_layer(attention_output)\n",
        "        \n",
        "        return logits"
      ],
      "metadata": {
        "id": "UVOQ43MjdKAt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing model"
      ],
      "metadata": {
        "id": "tOlRW2l7dGXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = torch.load('./BaseModel.zip', map_location=torch.device('cpu'))\n",
        "base_model.eval()\n",
        "\n",
        "new_model = torch.load('./NewModel.zip', map_location=torch.device('cpu'))\n",
        "new_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wYit3LYaTTe",
        "outputId": "db151931-43a7-4859-ac2b-11a61b4a3fc7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerLSTM(\n",
              "  (embedding): Embedding(53558, 120, padding_idx=1)\n",
              "  (positional_layer): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "  )\n",
              "  (transform_layer): ModuleList(\n",
              "    (0): TransformerLSTMCell(\n",
              "      (lstm): LSTM(120, 120, batch_first=True, dropout=0.1, bidirectional=True)\n",
              "      (feedfoward_layer_lstm): FeedForwardNetwork(\n",
              "        (linear_layer_1): Linear(in_features=120, out_features=28, bias=True)\n",
              "        (activation_layer): LeakyReLU(negative_slope=0.01)\n",
              "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
              "        (linear_layer_2): Linear(in_features=28, out_features=120, bias=True)\n",
              "      )\n",
              "      (multi_attention): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=120, out_features=120, bias=True)\n",
              "      )\n",
              "      (dropout_layer_1): Dropout(p=0.1, inplace=False)\n",
              "      (batchnorm_layer_1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
              "      (feedfoward_layer_multi): FeedForwardNetwork(\n",
              "        (linear_layer_1): Linear(in_features=120, out_features=28, bias=True)\n",
              "        (activation_layer): LeakyReLU(negative_slope=0.01)\n",
              "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
              "        (linear_layer_2): Linear(in_features=28, out_features=120, bias=True)\n",
              "      )\n",
              "      (dropout_layer_2): Dropout(p=0.1, inplace=False)\n",
              "      (batchnorm_layer_2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): TransformerLSTMCell(\n",
              "      (lstm): LSTM(120, 120, batch_first=True, dropout=0.1, bidirectional=True)\n",
              "      (feedfoward_layer_lstm): FeedForwardNetwork(\n",
              "        (linear_layer_1): Linear(in_features=120, out_features=28, bias=True)\n",
              "        (activation_layer): LeakyReLU(negative_slope=0.01)\n",
              "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
              "        (linear_layer_2): Linear(in_features=28, out_features=120, bias=True)\n",
              "      )\n",
              "      (multi_attention): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=120, out_features=120, bias=True)\n",
              "      )\n",
              "      (dropout_layer_1): Dropout(p=0.1, inplace=False)\n",
              "      (batchnorm_layer_1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
              "      (feedfoward_layer_multi): FeedForwardNetwork(\n",
              "        (linear_layer_1): Linear(in_features=120, out_features=28, bias=True)\n",
              "        (activation_layer): LeakyReLU(negative_slope=0.01)\n",
              "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
              "        (linear_layer_2): Linear(in_features=28, out_features=120, bias=True)\n",
              "      )\n",
              "      (dropout_layer_2): Dropout(p=0.1, inplace=False)\n",
              "      (batchnorm_layer_2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): TransformerLSTMCell(\n",
              "      (lstm): LSTM(120, 120, batch_first=True, dropout=0.1, bidirectional=True)\n",
              "      (feedfoward_layer_lstm): FeedForwardNetwork(\n",
              "        (linear_layer_1): Linear(in_features=120, out_features=28, bias=True)\n",
              "        (activation_layer): LeakyReLU(negative_slope=0.01)\n",
              "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
              "        (linear_layer_2): Linear(in_features=28, out_features=120, bias=True)\n",
              "      )\n",
              "      (multi_attention): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=120, out_features=120, bias=True)\n",
              "      )\n",
              "      (dropout_layer_1): Dropout(p=0.1, inplace=False)\n",
              "      (batchnorm_layer_1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
              "      (feedfoward_layer_multi): FeedForwardNetwork(\n",
              "        (linear_layer_1): Linear(in_features=120, out_features=28, bias=True)\n",
              "        (activation_layer): LeakyReLU(negative_slope=0.01)\n",
              "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
              "        (linear_layer_2): Linear(in_features=28, out_features=120, bias=True)\n",
              "      )\n",
              "      (dropout_layer_2): Dropout(p=0.1, inplace=False)\n",
              "      (batchnorm_layer_2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): TransformerLSTMCell(\n",
              "      (lstm): LSTM(120, 120, batch_first=True, dropout=0.1, bidirectional=True)\n",
              "      (feedfoward_layer_lstm): FeedForwardNetwork(\n",
              "        (linear_layer_1): Linear(in_features=120, out_features=28, bias=True)\n",
              "        (activation_layer): LeakyReLU(negative_slope=0.01)\n",
              "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
              "        (linear_layer_2): Linear(in_features=28, out_features=120, bias=True)\n",
              "      )\n",
              "      (multi_attention): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=120, out_features=120, bias=True)\n",
              "      )\n",
              "      (dropout_layer_1): Dropout(p=0.1, inplace=False)\n",
              "      (batchnorm_layer_1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
              "      (feedfoward_layer_multi): FeedForwardNetwork(\n",
              "        (linear_layer_1): Linear(in_features=120, out_features=28, bias=True)\n",
              "        (activation_layer): LeakyReLU(negative_slope=0.01)\n",
              "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
              "        (linear_layer_2): Linear(in_features=28, out_features=120, bias=True)\n",
              "      )\n",
              "      (dropout_layer_2): Dropout(p=0.1, inplace=False)\n",
              "      (batchnorm_layer_2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (output_layer): Linear(in_features=120, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Inference"
      ],
      "metadata": {
        "id": "PwaHdru1eMAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input painting description\n",
        "text = \"the student can finally rest after finishing the work\"\n",
        "text = tokenizer(text)\n",
        "text = vocab(text)\n",
        "text = torch.IntTensor(text)\n",
        "text = text.reshape(1, -1)\n",
        "\n",
        "#Prediction on the description\n",
        "prediction = base_model(text).argmax(1)\n",
        "print(\"base model's prediction: \" + label_dict[prediction.item()])\n",
        "\n",
        "prediction = new_model(text).argmax(1)\n",
        "print(\"new model's prediction: \" + label_dict[prediction.item()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxeQ_92AdARw",
        "outputId": "ad79e7dd-3d90-4dde-cb52-b200a228a356"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base model's prediction: contentment\n",
            "new model's prediction: awe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input painting description\n",
        "text = \"the student is hopeful that he can get a passing grade\"\n",
        "text = tokenizer(text)\n",
        "text = vocab(text)\n",
        "text = torch.IntTensor(text)\n",
        "text = text.reshape(1, -1)\n",
        "\n",
        "#Prediction on the description\n",
        "prediction = base_model(text).argmax(1)\n",
        "print(\"base model's prediction: \" + label_dict[prediction.item()])\n",
        "\n",
        "prediction = new_model(text).argmax(1)\n",
        "print(\"new model's prediction: \" + label_dict[prediction.item()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KID7CmdGkEby",
        "outputId": "8bb33d9f-bc16-43c4-8a99-3debc4edd60e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base model's prediction: something else\n",
            "new model's prediction: contentment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input painting description\n",
        "text = \"the street is empty and lonely because of the pandemic\"\n",
        "text = tokenizer(text)\n",
        "text = vocab(text)\n",
        "text = torch.IntTensor(text)\n",
        "text = text.reshape(1, -1)\n",
        "\n",
        "#Prediction on the description\n",
        "prediction = base_model(text).argmax(1)\n",
        "print(\"base model's prediction: \" + label_dict[prediction.item()])\n",
        "\n",
        "prediction = new_model(text).argmax(1)\n",
        "print(\"new model's prediction: \" + label_dict[prediction.item()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UgkQX0RkElg",
        "outputId": "4a5cb40b-5def-445a-f86a-08a917aace09"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base model's prediction: fear\n",
            "new model's prediction: sadness\n"
          ]
        }
      ]
    }
  ]
}